1. download the model into default cache:
        model_name = "model name in HF"
        # Load the tokenizer and model
        model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, safe_serialization=False)
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, safe_serialization=False)

2. save model locally:
        model_path = './model'
        model = AutoModelForCausalLM.from_pretrained(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model.save_pretrained(model_path)
        tokenizer.save_pretrained(model_path)

3. query model, find the appropriate chat model on the model page on HF
        model_path = './model'
        model = AutoModelForCausalLM.from_pretrained(model_path)
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        # Sample input for conversation

        input_text = "Hello, when and where the first olympic games happened?"

        # Tokenize the input
        input_ids = tokenizer.encode(input_text, return_tensors="pt")

        # Generate the model's response
        output_ids = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.eos_token_id)

        # Decode and print the response
        response = tokenizer.decode(output_ids[0], skip_special_tokens=True)

        print("Response from the model:")
        print(response)


